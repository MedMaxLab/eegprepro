{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78dde8eb-a1d0-4f84-ba8d-7f0c1400bd46",
   "metadata": {},
   "source": [
    "# Split Analysis\n",
    "\n",
    "This notebook replicate all the Nested Leave-N-Subject-Out splits\n",
    "(10 outer folds and 5 inner) and sum up some information such as:\n",
    "1. sets ratios (should be near 0.72 for train, 0.18 for validation, 0.10 for test).\n",
    "2. class ratios for each sets (should preserve total class ratios).\n",
    "3. hypothetical baseline classifier's unbalanced accuracy (weighted random guess).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c7c13-2fbc-4d31-b208-9fd548f61fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message = \"Using padding='same'\", category = UserWarning\n",
    ")\n",
    "\n",
    "# IMPORT STANDARD PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORT SELFEEG \n",
    "import selfeeg\n",
    "import selfeeg.dataloading as dl\n",
    "\n",
    "# IMPORT REPOSITORY FUNCTIONS\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from AllFnc import split\n",
    "from AllFnc.training import loadEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbf79a-eba6-4665-9696-284a35ab56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_split2(\n",
    "    EEGlen: pd.DataFrame,\n",
    "    EEGsplit: pd.DataFrame,\n",
    "    Labels=None,\n",
    "    return_ratio=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    # Check split ratio\n",
    "    # simply the ratio between the sum of all samples with a specific label set\n",
    "    # and the sum of all samples with label different from -1\n",
    "    total_list = EEGsplit[EEGsplit[\"split_set\"] != -1].index.tolist()\n",
    "    total = EEGlen.iloc[total_list][\"N_samples\"].sum()\n",
    "    train_list = EEGsplit[EEGsplit[\"split_set\"] == 0].index.tolist()\n",
    "    train_ratio = EEGlen.iloc[train_list][\"N_samples\"].sum() / total\n",
    "    val_list = EEGsplit[EEGsplit[\"split_set\"] == 1].index.tolist()\n",
    "    val_ratio = EEGlen.iloc[val_list][\"N_samples\"].sum() / total\n",
    "    test_list = EEGsplit[EEGsplit[\"split_set\"] == 2].index.tolist()\n",
    "    test_ratio = EEGlen.iloc[test_list][\"N_samples\"].sum() / total\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\ntrain ratio:      {train_ratio:.2f}\")\n",
    "        print(f\"validation ratio: {val_ratio:.2f}\")\n",
    "        print(f\"test ratio:       {test_ratio:.2f}\")\n",
    "    ratios = {\n",
    "        \"train_total\": EEGlen.iloc[train_list][\"N_samples\"].sum(), \"train_ratio\": train_ratio,\n",
    "        \"val_total\": EEGlen.iloc[val_list][\"N_samples\"].sum(), \"val_ratio\": val_ratio,\n",
    "        \"test_total\": EEGlen.iloc[test_list][\"N_samples\"].sum(), \"test_ratio\": test_ratio,\n",
    "        \"class_ratio\": None,\n",
    "        \"class_total\": None,\n",
    "    }\n",
    "\n",
    "    # Check class ratio\n",
    "    # similar to the previous one but the ratios are calculated with respect to\n",
    "    # the subset sizes (train test validation sets)\n",
    "    if Labels is not None:\n",
    "        Labels = np.array(Labels)\n",
    "        if len(Labels.shape) != 1:\n",
    "            raise ValueError(\"Labels must be a 1d array or a list\")\n",
    "        lab_unique = np.unique(Labels)\n",
    "        EEGlen2 = EEGlen.copy()  # copy to avoid strange behaviours\n",
    "        EEGlen2[\"split_set\"] = EEGsplit[\"split_set\"]\n",
    "        EEGlen2[\"Labels\"] = Labels\n",
    "        tottrain = EEGlen2.iloc[train_list][\"N_samples\"].sum()\n",
    "        totval = EEGlen2.iloc[val_list][\"N_samples\"].sum()\n",
    "        tottest = EEGlen2.iloc[test_list][\"N_samples\"].sum()\n",
    "        class_ratio = np.zeros((3, len(lab_unique)))\n",
    "        class_total = np.zeros((3, len(lab_unique)))\n",
    "        # iterate through train/validation/test sets\n",
    "        for i in range(3):\n",
    "            # iterate through each label\n",
    "            for k in range(len(lab_unique)):\n",
    "                if i == 0:\n",
    "                    train_k = EEGlen2.loc[\n",
    "                        ((EEGlen2[\"split_set\"] == 0) & (EEGlen2[\"Labels\"] == lab_unique[k])),\n",
    "                        \"N_samples\",\n",
    "                    ].sum()\n",
    "                    class_ratio[i, k] = train_k / tottrain\n",
    "                    class_total[i, k] = train_k\n",
    "                elif i == 1:\n",
    "                    val_k = EEGlen2.loc[\n",
    "                        ((EEGlen2[\"split_set\"] == 1) & (EEGlen2[\"Labels\"] == lab_unique[k])),\n",
    "                        \"N_samples\",\n",
    "                    ].sum()\n",
    "                    class_ratio[i, k] = val_k / totval\n",
    "                    class_total[i, k] = val_k\n",
    "                else:\n",
    "                    test_k = EEGlen2.loc[\n",
    "                        ((EEGlen2[\"split_set\"] == 2) & (EEGlen2[\"Labels\"] == lab_unique[k])),\n",
    "                        \"N_samples\",\n",
    "                    ].sum()\n",
    "                    class_ratio[i, k] = test_k / tottest\n",
    "                    class_total[i, k] = test_k\n",
    "        # print results\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\ntrain labels ratio:\",\n",
    "                *[f\"{lab_unique[k]}={class_ratio[0,k]:.3f}, \" for k in range(len(lab_unique))],\n",
    "            )\n",
    "            print(\n",
    "                f\"val   labels ratio:\",\n",
    "                *[f\"{lab_unique[k]}={class_ratio[1,k]:.3f}, \" for k in range(len(lab_unique))],\n",
    "            )\n",
    "            print(\n",
    "                f\"test  labels ratio:\",\n",
    "                *[f\"{lab_unique[k]}={class_ratio[2,k]:.3f}, \" for k in range(len(lab_unique))],\n",
    "            )\n",
    "            print(\"\")\n",
    "        ratios[\"class_ratio\"] = class_ratio\n",
    "        ratios[\"class_total\"] = class_total\n",
    "    \n",
    "    # return calculated ratios if necessary\n",
    "    if return_ratio:\n",
    "        return ratios\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c38f4-f97a-4051-9bc4-ccc2b5edd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath       = '/data/delpup/eegpickle/'\n",
    "pipelineToEval = 'filt'\n",
    "tasksToEval     = ['alzheimer', 'eyes', 'parkinson', 'motorimagery', 'sleep', 'psychosis']\n",
    "downsample     = True\n",
    "z_score        = True\n",
    "rem_interp     = False\n",
    "overlap        = 0.0\n",
    "verbose        = True\n",
    "\n",
    "summary_table = np.zeros((50*6, 30))\n",
    "for k, taskToEval in enumerate(tasksToEval):\n",
    "    if taskToEval.casefold() == 'eyes':\n",
    "        partition_list = split.create_nested_kfold_subject_split(60,10,5)\n",
    "    \n",
    "    elif taskToEval.casefold() == 'sleep':\n",
    "        partition_list = split.create_nested_kfold_subject_split(71,10,5)\n",
    "    \n",
    "    elif taskToEval.casefold() == 'alzheimer':\n",
    "        # ALZ = subjects 1 to 36; CTL = subjects 37 to 65; FTD = subjects 66 to 88\n",
    "        part_a = split.create_nested_kfold_subject_split([i for i in range(1,37)], 10, 5)\n",
    "        part_c = split.create_nested_kfold_subject_split([i for i in range(37,66)], 10, 5)\n",
    "        part_f = split.create_nested_kfold_subject_split([i for i in range(66,89)], 10, 5)\n",
    "        partition_list_1 = split.merge_partition_lists(part_a, part_c, 10, 5)\n",
    "        partition_list = split.merge_partition_lists(partition_list_1, part_f, 10, 5)\n",
    "    \n",
    "    elif taskToEval.casefold() == 'parkinson':\n",
    "        # In this case, two datasets were merged to increase the number \n",
    "        # of subjects. So, there are two partition lists to create\n",
    "    \n",
    "        #ds003490 - ID 5 - 3Stim\n",
    "        ctl_id = [i for i in range(28,51)] + [3,5]\n",
    "        pds_id = [i for i in range(6,28)] + [1,2,4]\n",
    "        part_c = split.create_nested_kfold_subject_split(ctl_id, 10, 5)\n",
    "        part_p = split.create_nested_kfold_subject_split(pds_id, 10, 5)\n",
    "        partition_list_1 = split.merge_partition_lists(part_c, part_p, 10, 5)\n",
    "    \n",
    "        #ds002778 - ID 8 - UCSD\n",
    "        part_c = split.create_nested_kfold_subject_split([i for i in range(1,17)], 10, 5)\n",
    "        part_p = split.create_nested_kfold_subject_split([i for i in range(17,32)], 10, 5)\n",
    "        partition_list_2 = split.merge_partition_lists(part_c, part_p, 10, 5)\n",
    "    \n",
    "    elif taskToEval.casefold() == 'psychosis':\n",
    "        # CTL = subjects 101 to 149; PD/PDD/PDMCI = mixing number in [1; 100]\n",
    "        ctl_id = [1, 3, 4, 9, 10, 12, 13, 14, 17, 19, 21, 22, 24, 25, 27, 29, 30,\n",
    "                  31, 35, 38, 39, 41, 43, 46, 48, 49, 53, 55, 58, 59]\n",
    "        psy_id = [2, 5, 6, 7, 8, 11, 15, 16, 18, 20, 23, 26, 28, 32, 33, 34, 36,\n",
    "                  37, 40, 42, 44, 45, 47, 50, 51, 52, 54, 56, 57, 60, 61]\n",
    "        part_c = split.create_nested_kfold_subject_split(ctl_id, 10, 5)\n",
    "        part_p = split.create_nested_kfold_subject_split(psy_id, 10, 5)\n",
    "        partition_list = split.merge_partition_lists(part_c, part_p, 10, 5)       \n",
    "    else:\n",
    "        # three subjects were excluded for the known issue of having \n",
    "        # a sampling rate of 128 Hz instead of 160 Hz (and strange trial length).\n",
    "        subject_list = [i for i in range(1,110) if i not in [88,92,100]] \n",
    "        partition_list = split.create_nested_kfold_subject_split(subject_list,10,5)\n",
    "    \n",
    "    N_subj = len(partition_list[0][0]) + len(partition_list[0][1]) + len(partition_list[0][2])\n",
    "    # Section 4: set the training parameters\n",
    "    \n",
    "    if dataPath[-1] != os.sep:\n",
    "        dataPath += os.sep\n",
    "    if pipelineToEval[-1] != os.sep:\n",
    "        eegpath = dataPath + pipelineToEval + os.sep\n",
    "    else:\n",
    "        eegpath = dataPath + pipelineToEval\n",
    "    \n",
    "    if taskToEval.casefold() == 'motorimagery':\n",
    "        freq = 160\n",
    "    else:\n",
    "        freq = 125 if downsample else 250\n",
    "    \n",
    "    # Define the partition window length, in second. 4s for every task except the MI\n",
    "    # which has samples of 4.1s due to the sampling rate\n",
    "    window = 4.1 if taskToEval.casefold() == 'motorimagery' else 4.0\n",
    "    \n",
    "    # Define the number of classes to predict.\n",
    "    # All tasks are binary except the Alzheimer's one, \n",
    "    # which is a multi-class classification (Alzheimer vs FrontoTemporal vs Control)\n",
    "    if taskToEval.casefold() == 'alzheimer':\n",
    "        nb_classes = 3\n",
    "    else:\n",
    "        nb_classes = 2\n",
    "    \n",
    "    # For selfEEG's models instantiation\n",
    "    Samples = int(freq*window)\n",
    "    \n",
    "    # Set the Dataset ID for glob.glob operation in SelfEEG's GetEEGPartitionNumber().\n",
    "    # It is a single number for every task except for PD that merges two datasets\n",
    "    if taskToEval.casefold() == 'eyes':\n",
    "        datasetID = '2'\n",
    "    elif taskToEval.casefold() == 'alzheimer':\n",
    "        datasetID = '10'\n",
    "    elif taskToEval.casefold() == 'motorimagery':\n",
    "        datasetID = '25'\n",
    "    elif taskToEval.casefold() == 'sleep':\n",
    "        datasetID = '20'\n",
    "    elif taskToEval.casefold() == 'psychosis':\n",
    "        datasetID = '7'\n",
    "    else:\n",
    "        datasetID_1 = '5' # EEG 3-Stim\n",
    "        datasetID_2 = '8' # UC SD\n",
    "    \n",
    "    #  Section 5: Define pytorch's Datasets and dataloaders\n",
    "    \n",
    "    loadEEG_args = {\n",
    "        'return_label': False, \n",
    "        'downsample': downsample, \n",
    "        'use_only_original': rem_interp,\n",
    "        'eegsym_train': False,\n",
    "        'apply_zscore': z_score\n",
    "    }\n",
    "    \n",
    "    if taskToEval.casefold() == 'parkinson':\n",
    "        glob_input = [datasetID_1 + '_*.pickle', datasetID_2 + '_*.pickle' ]\n",
    "    else:\n",
    "        glob_input = [datasetID + '_*.pickle']\n",
    "    \n",
    "    # calculate dataset length.\n",
    "    # Basically it automatically retrieves all the partitions \n",
    "    # that can be extracted from each EEG signal\n",
    "    EEGlen = dl.get_eeg_partition_number(\n",
    "        eegpath,\n",
    "        freq,\n",
    "        window,\n",
    "        overlap, \n",
    "        file_format = glob_input,\n",
    "        load_function = loadEEG,\n",
    "        optional_load_fun_args = loadEEG_args,\n",
    "        includePartial = False if overlap == 0 else True,\n",
    "        verbose = verbose\n",
    "    )\n",
    "    \n",
    "    # Now we also need to load the labels\n",
    "    loadEEG_args['return_label'] = True\n",
    "    \n",
    "    # Set functions to retrieve dataset, subject, and session from each filename.\n",
    "    # They will be used by GetEEGSplitTable to perform a subject based split\n",
    "    dataset_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[0])\n",
    "    subject_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[1]) \n",
    "    session_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[2]) \n",
    "\n",
    "    labels = np.zeros(len(EEGlen))\n",
    "    for i in range(len(EEGlen)):\n",
    "        path = EEGlen.iloc[i,0]\n",
    "        with open(path, 'rb') as eegfile:\n",
    "            EEG = pickle.load(eegfile)\n",
    "        labels[i] = EEG['label']\n",
    "    \n",
    "    for outerFold in range(10):\n",
    "        for innerFold in range(5):\n",
    "            \n",
    "            # fold to eval is the correct index to get the desired train/val/test partition\n",
    "            foldToEval = outerFold*5 + innerFold\n",
    "\n",
    "            N_subj_train = len(partition_list[foldToEval][0])\n",
    "            N_subj_val = len(partition_list[foldToEval][1])\n",
    "            N_subj_test = len(partition_list[foldToEval][2])\n",
    "            \n",
    "            # Now call the GetEEGSplitTable. Since Parkinson task merges two datasets\n",
    "            # we need to differentiate between this and other tasks\n",
    "            if taskToEval.casefold() == 'parkinson':\n",
    "                # Remember\n",
    "                # 1 --> 5 = EEG 3-Stim  2 --> 8 = UCSD\n",
    "                train_id   = { 5: partition_list_1[foldToEval][0], \n",
    "                               8: partition_list_2[foldToEval][0]}\n",
    "                val_id     = { 5: partition_list_1[foldToEval][1], \n",
    "                               8: partition_list_2[foldToEval][1]}\n",
    "                test_id    = { 5: partition_list_1[foldToEval][2],\n",
    "                               8: partition_list_2[foldToEval][2]}\n",
    "                EEGsplit= dl.get_eeg_split_table(\n",
    "                    partition_table      = EEGlen,\n",
    "                    exclude_data_id      = None,  #[8], just checked if UCSD was useful \n",
    "                    val_data_id          = val_id,\n",
    "                    test_data_id         = test_id, \n",
    "                    split_tolerance      = 0.001,\n",
    "                    dataset_id_extractor = dataset_id_ex,\n",
    "                    subject_id_extractor = subject_id_ex,\n",
    "                    perseverance         = 10000\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                train_id   = partition_list[foldToEval][0]\n",
    "                val_id     = partition_list[foldToEval][1]\n",
    "                test_id    = partition_list[foldToEval][2]\n",
    "                EEGsplit= dl.get_eeg_split_table(\n",
    "                    partition_table      = EEGlen,\n",
    "                    exclude_data_id      = None,\n",
    "                    val_data_id          = val_id,\n",
    "                    test_data_id         = test_id, \n",
    "                    split_tolerance      = 0.001,\n",
    "                    dataset_id_extractor = subject_id_ex,\n",
    "                    subject_id_extractor = session_id_ex,\n",
    "                    perseverance         = 10000\n",
    "                )\n",
    "            \n",
    "            # Define Datasets and preload all data\n",
    "            trainset = dl.EEGDataset(\n",
    "                EEGlen, EEGsplit, [freq, window, overlap], 'train', \n",
    "                supervised             = True, \n",
    "                label_on_load          = True,\n",
    "                load_function          = loadEEG,\n",
    "                optional_load_fun_args = loadEEG_args\n",
    "            )\n",
    "            \n",
    "            valset = dl.EEGDataset(\n",
    "                EEGlen, EEGsplit, [freq, window, overlap], 'validation',\n",
    "                supervised             = True, \n",
    "                label_on_load          = True,\n",
    "                load_function          = loadEEG,\n",
    "                optional_load_fun_args = loadEEG_args\n",
    "            )\n",
    "            \n",
    "            testset = dl.EEGDataset(\n",
    "                EEGlen, EEGsplit, [freq, window, overlap], 'test',\n",
    "                supervised             = True,\n",
    "                label_on_load          = True,\n",
    "                load_function          = loadEEG,\n",
    "                optional_load_fun_args = loadEEG_args\n",
    "            )\n",
    "            ratios = check_split2(EEGlen, EEGsplit, labels, return_ratio = True, verbose = False)\n",
    "            idx = k*50 + foldToEval\n",
    "            summary_table[idx, :4] = np.array([N_subj, N_subj_train, N_subj_val, N_subj_test])\n",
    "            summary_table[idx, 4:8] = np.array([len(trainset)+len(valset)+len(testset) ,\n",
    "                                           len(trainset), len(valset), len(testset)  ])\n",
    "            summary_table[idx, 8:11] = np.array([ratios['train_ratio'],\n",
    "                                            ratios['val_ratio'],\n",
    "                                            ratios['test_ratio']])\n",
    "            if taskToEval == 'alzheimer':\n",
    "                nb = 3\n",
    "            else:\n",
    "                nb = 2\n",
    "                \n",
    "            summary_table[idx, 11:11+nb] =  ratios['class_total'][0,:]\n",
    "            summary_table[idx, 14:14+nb] =  ratios['class_ratio'][0,:]\n",
    "            summary_table[idx, 17:17+nb] =  ratios['class_total'][1,:]\n",
    "            summary_table[idx, 20:20+nb] =  ratios['class_ratio'][1,:]\n",
    "            summary_table[idx, 23:23+nb] =  ratios['class_total'][2,:]\n",
    "            summary_table[idx, 26:26+nb] =  ratios['class_ratio'][2,:]\n",
    "            summary_table[idx, 29] =  (ratios['class_ratio'][2,:]**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194801cb-fa5d-4cb0-a6c5-6394cad53b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Theoretical Baseline - Weighted Random Choice - Accuracy Unbalanced Metric')\n",
    "print('--------------------------------------------------------------------------')\n",
    "for i in range(6):\n",
    "    print(' ')\n",
    "    if i==0:\n",
    "        print(' Alzheimer vs FrontoTemporal Dementia vs Control')\n",
    "    elif i==1:\n",
    "        print('             Eyes Open Eyes Closed')\n",
    "    elif i==2:\n",
    "        print('              Parkinson vs Control')\n",
    "    elif i==3:\n",
    "        print('                 Motor Imagery')\n",
    "    elif i==4:\n",
    "        print('               Sleep Deprivation')\n",
    "    elif i==5:\n",
    "        print('       First Episode Psychosis vs Control')\n",
    "    m1=np.median(summary_table[50*i:50*(i+1),-1])\n",
    "    m2=np.percentile(summary_table[50*i:50*(i+1),-1], 75)\n",
    "    m3=np.percentile(summary_table[50*i:50*(i+1),-1], 25)\n",
    "    print( ' --------------------------------------------------')\n",
    "    print( \"|  first quartile  |   median   |  third quartile  |\")\n",
    "    print(f\"|       {m3:5.3f}      |    {m1:5.3f}   |       {m2:5.3f}      |\")\n",
    "    print( ' --------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1429468-237e-4204-9ce3-b444fe2a2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table_df = pd.DataFrame(\n",
    "    summary_table,\n",
    "    columns=[\n",
    "        'subj_tot', 'subj_train', 'subj_val', 'subj_test',\n",
    "        'sample_tot', 'sample_train', 'sample_val', 'sample_test', \n",
    "        'ratio_train', 'ratio_val', 'ratio_test',\n",
    "        'sample_train_class_0', 'sample_train_class_1', 'sample_train_class_2',\n",
    "        'ratio_train_class_0', 'ratio_train_class_1', 'ratio_train_class_2',\n",
    "        'sample_val_class_0', 'sample_val_class_1', 'sample_val_class_2', \n",
    "        'ratio_val_class_0', 'ratio_val_class_1', 'ratio_val_class_2',\n",
    "        'sample_test_class_0', 'sample_test_class_1', 'sample_test_class_2', \n",
    "        'ratio_test_class_0', 'ratio_test_class_1', 'ratio_test_class_2',\n",
    "        'baseline'\n",
    "    ]\n",
    ")\n",
    "new_col = [([i]*50) for i in tasksToEval]\n",
    "new_col = [x for xs in new_col for x in xs]\n",
    "summary_table_df.insert(loc=0, column='task', value=new_col)\n",
    "if pipelineToEval in ['raw', 'filt']:\n",
    "    summary_table_df.to_csv('split_analysis_raw_and_filt.csv', index=False)\n",
    "else:\n",
    "    summary_table_df.to_csv('split_analysis_ica_and_isr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f4f6e-8c56-40eb-ae1c-083a8eb34d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipelineToEval in ['raw', 'filt']:\n",
    "    data = pd.read_csv('split_analysis_raw_and_filt.csv')\n",
    "else:\n",
    "    data = pd.read_csv('split_analysis_ica_and_isr.csv')\n",
    "\n",
    "# --------------------------------\n",
    "# | CHOOSE THE TASK TO SUMMARIZE |\n",
    "# --------------------------------\n",
    "task = 'alzheimer'\n",
    "# --------------------------------\n",
    "\n",
    "Qlow = 0.05\n",
    "Qhigh = 0.95\n",
    "print('------------------------------------------------------')\n",
    "print('                     TASK:', task.upper())\n",
    "\n",
    "# get rows with task\n",
    "data2 = data.loc[data['task']==task].reset_index().drop(columns=['index'])\n",
    "\n",
    "# class ratio\n",
    "zero_ratio = data2.iloc[0,[12,18,24]].sum()/data2.iloc[0,5]\n",
    "one_ratio = data2.iloc[0,[13,19,25]].sum()/data2.iloc[0,5]\n",
    "two_ratio = data2.iloc[0,[14,20,26]].sum()/data2.iloc[0,5]\n",
    "print('------------------------------------------------------')\n",
    "print('CLASS RATIO')\n",
    "print(f'0 = {zero_ratio:.3f}, 1 = {one_ratio:.3f}, 2 = {two_ratio:.3f}')\n",
    "\n",
    "# train ratio\n",
    "md_rt = data2.iloc[:,9].median()\n",
    "q1_rt = data2.iloc[:,9].quantile(Qlow)\n",
    "q3_rt = data2.iloc[:,9].quantile(Qhigh)\n",
    "print('------------------------------------------------------')\n",
    "print('TRAIN RATIO') \n",
    "print(f'median = {md_rt:.3f}, Q_low = {q1_rt:.3f}, Q_high = {q3_rt:.3f}')\n",
    "\n",
    "# val ratio\n",
    "md_rt = data2.iloc[:,10].median()\n",
    "q1_rt = data2.iloc[:,10].quantile(Qlow)\n",
    "q3_rt = data2.iloc[:,10].quantile(Qhigh)\n",
    "print('------------------------------------------------------')\n",
    "print('VALIDATION RATIO')\n",
    "print(f'median = {md_rt:.3f}, Q_low = {q1_rt:.3f}, Q_high = {q3_rt:.3f}')\n",
    "\n",
    "# test ratio\n",
    "md_rt = data2.iloc[:,11].median()\n",
    "q1_rt = data2.iloc[:,11].quantile(Qlow)\n",
    "q3_rt = data2.iloc[:,11].quantile(Qhigh)\n",
    "print('------------------------------------------------------')\n",
    "print('TEST  RATIO')\n",
    "print(f'median = {md_rt:.3f}, Q_low = {q1_rt:.3f}, Q_high = {q3_rt:.3f}')\n",
    "\n",
    "# train class ratio\n",
    "md_rt0 = data2.iloc[:,15].median()\n",
    "q1_rt0 = data2.iloc[:,15].quantile(Qlow)\n",
    "q3_rt0 = data2.iloc[:,15].quantile(Qhigh)\n",
    "md_rt1 = data2.iloc[:,16].median()\n",
    "q1_rt1 = data2.iloc[:,16].quantile(Qlow)\n",
    "q3_rt1 = data2.iloc[:,16].quantile(Qhigh)\n",
    "md_rt2 = data2.iloc[:,17].median()\n",
    "q1_rt2 = data2.iloc[:,17].quantile(Qlow)\n",
    "q3_rt2 = data2.iloc[:,17].quantile(Qhigh)\n",
    "print('------------------------------------------------------')\n",
    "print('TRAIN CLASS RATIO')\n",
    "print(f'CLASS 0: median = {md_rt0:.3f}, Q_low = {q1_rt0:.3f}, Q_high = {q3_rt0:.3f}')\n",
    "print(f'CLASS 1: median = {md_rt1:.3f}, Q_low = {q1_rt1:.3f}, Q_high = {q3_rt1:.3f}')\n",
    "print(f'CLASS 2: median = {md_rt2:.3f}, Q_low = {q1_rt2:.3f}, Q_high = {q3_rt2:.3f}')\n",
    "\n",
    "# val class ratio\n",
    "md_rt0 = data2.iloc[:,21].median()\n",
    "q1_rt0 = data2.iloc[:,21].quantile(Qlow)\n",
    "q3_rt0 = data2.iloc[:,21].quantile(Qhigh)\n",
    "md_rt1 = data2.iloc[:,22].median()\n",
    "q1_rt1 = data2.iloc[:,22].quantile(Qlow)\n",
    "q3_rt1 = data2.iloc[:,22].quantile(Qhigh)\n",
    "md_rt2 = data2.iloc[:,23].median()\n",
    "q1_rt2 = data2.iloc[:,23].quantile(Qlow)\n",
    "q3_rt2 = data2.iloc[:,23].quantile(Qhigh)\n",
    "print('------------------------------------------------------')\n",
    "print('VALIDATION CLASS RATIO')\n",
    "print(f'CLASS 0: median = {md_rt0:.3f}, Q_low = {q1_rt0:.3f}, Q_high = {q3_rt0:.3f}')\n",
    "print(f'CLASS 1: median = {md_rt1:.3f}, Q_low = {q1_rt1:.3f}, Q_high = {q3_rt1:.3f}')\n",
    "print(f'CLASS 2: median = {md_rt2:.3f}, Q_low = {q1_rt2:.3f}, Q_high = {q3_rt2:.3f}')\n",
    "\n",
    "# train class ratio\n",
    "md_rt0 = data2.iloc[:,27].median()\n",
    "q1_rt0 = data2.iloc[:,27].quantile(Qlow)\n",
    "q3_rt0 = data2.iloc[:,27].quantile(Qhigh)\n",
    "md_rt1 = data2.iloc[:,28].median()\n",
    "q1_rt1 = data2.iloc[:,28].quantile(Qlow)\n",
    "q3_rt1 = data2.iloc[:,28].quantile(Qhigh)\n",
    "md_rt2 = data2.iloc[:,29].median()\n",
    "q1_rt2 = data2.iloc[:,29].quantile(Qlow)\n",
    "q3_rt2 = data2.iloc[:,29].quantile(Qhigh)\n",
    "print('------------------------------------------------------')\n",
    "print('TEST CLASS RATIO')\n",
    "print(f'CLASS 0: median = {md_rt0:.3f}, Q_low = {q1_rt0:.3f}, Q_high = {q3_rt0:.3f}')\n",
    "print(f'CLASS 1: median = {md_rt1:.3f}, Q_low = {q1_rt1:.3f}, Q_high = {q3_rt1:.3f}')\n",
    "print(f'CLASS 2: median = {md_rt2:.3f}, Q_low = {q1_rt2:.3f}, Q_high = {q3_rt2:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
