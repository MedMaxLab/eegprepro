{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2355bf6e-b853-43f5-90e0-65ca0e0e6d98",
   "metadata": {},
   "source": [
    "# Statistical Analysis on different Pipelines\n",
    "\n",
    "This notebook implements all the figures shown in the original paper.\n",
    "\n",
    "To run it, you must create a **custom environment for statannotations**,\n",
    "which unfortunately has messed up the dependecy list.\n",
    "This issue was solved by creating a python environment with python 3.11,\n",
    "then by installing seaborn 0.12, and finally statannotations with the --no-dependencies\n",
    "option.\n",
    "\n",
    "You also need to run the python script RunKfoldAll.py to train all the models.\n",
    "Alternatively, we provide a list of pickle files (highly compressed with zpaq) with the\n",
    "result got from of every single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb986723-2995-4651-a749-e3fae9e788b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# Set Style and Warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Set Path\n",
    "sep = os.path.sep\n",
    "current_folder1    = os.getcwd()\n",
    "current_folder     = '/home/delpup/EEG_SSL_Project'\n",
    "\n",
    "root_folder_name   = 'eegprepro'\n",
    "images_folder_name = 'Images'\n",
    "\n",
    "root_folder    = current_folder + sep + root_folder_name   + sep\n",
    "images_folder  = current_folder1 + sep + images_folder_name + sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc203a7d-af71-494c-ac0a-ac24800beb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLearningRateString(model, task):\n",
    "    model_conversion_dict = {\n",
    "        'egn': 'eegnet',\n",
    "        'shn': 'shallownet',\n",
    "        'dcn': 'deepconvnet',\n",
    "        'fbc': 'fbcnet'\n",
    "    }\n",
    "    task_conversion_dict = {\n",
    "        'eye': 'eyes',\n",
    "        'alz': 'alzheimer',\n",
    "        'mmi': 'motorimagery',\n",
    "        'pds': 'parkinson',\n",
    "        'slp': 'sleep',\n",
    "        'fep': 'psychosis'\n",
    "    }\n",
    "    if len(model)==3:\n",
    "        model = model_conversion_dict.get(model)\n",
    "    if len(task)==3:\n",
    "        task = task_conversion_dict.get(task)\n",
    "    lr_dict = {\n",
    "        'eegnet': {\n",
    "            'eyes': 5e-04,\n",
    "            'parkinson': 1e-04,\n",
    "            'alzheimer': 7.5e-04,\n",
    "            'motorimagery': 1e-03,\n",
    "            'sleep': 1e-03,\n",
    "            'psychosis': 1e-04\n",
    "        },\n",
    "        'shallownet': {\n",
    "            'eyes': 1e-03,\n",
    "            'parkinson': 2.5e-04, #2.5e-05\n",
    "            'alzheimer': 5e-05,\n",
    "            'motorimagery': 7.5e-04,\n",
    "            'sleep': 5e-05,\n",
    "            'psychosis': 7.5e-05\n",
    "        },\n",
    "        'deepconvnet': {\n",
    "            'eyes': 7.5e-04,\n",
    "            'parkinson': 2.5e-04,\n",
    "            'alzheimer': 7.5e-04,\n",
    "            'motorimagery': 7.5e-04,\n",
    "            'sleep': 2.5e-04,\n",
    "            'psychosis': 1e-03\n",
    "        },\n",
    "        'fbcnet': {\n",
    "            'eyes': 7.5e-04,\n",
    "            'parkinson': 2.5e-04,\n",
    "            'alzheimer': 7.5e-05,\n",
    "            'motorimagery': 1e-3,\n",
    "            'sleep': 1e-04,\n",
    "            'psychosis': 1e-05\n",
    "        }\n",
    "    }\n",
    "    lr = lr_dict.get(model).get(task)\n",
    "    lr = str(int(lr*1e6)).zfill(6)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4a60a-c322-4ac3-bc31-cfb0798db75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "task_folder = ['Eoec','MI','PD','Alz','Sleep','FEP']\n",
    "task_names  = ['eye','mmi','pds','alz','slp','fep']\n",
    "models      = ['egn','shn','dcn','fbc']\n",
    "pipelines   = ['raw','flt','ica','isr']\n",
    "srate       = [125,250]\n",
    "verbose     = False\n",
    "\n",
    "out_folder = 10\n",
    "in_folder = 5\n",
    "pth = 0.05\n",
    "\n",
    "# Set Conversion Dictionary for Plots\n",
    "convert_dict = {'eye': 'Eye',\n",
    "                'mmi': 'MMI',\n",
    "                'pds': 'Parkinson',\n",
    "                'alz': 'Alzheimer',\n",
    "                'slp': 'Sleep',\n",
    "                'fep': 'FEP',\n",
    "                'egn': 'EEGNet',\n",
    "                'shn': 'ShallowNet',\n",
    "                'dcn': 'DeepConvNet',\n",
    "                'fbc': 'FBCNet',\n",
    "                'raw': 'Raw',\n",
    "                'flt': 'Filt',\n",
    "                'ica': 'ICA',\n",
    "                'isr': 'ICA+ASR',\n",
    "                'accuracy_unbalanced': 'Unbalanced Accuracy',\n",
    "                'accuracy_weighted'  : 'Balanced Accuracy',\n",
    "                'f1score_weighted'   : 'F1-score Weighted',\n",
    "                'precision_weighted' : 'Precision Weighted',\n",
    "                'recall_weighted'    : 'Recall Wighted'                \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4324eca-2131-40fc-bcd9-3f4f950140dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_acc(root_folder, numbers, task_folder, task_names, pipelines, models, srate, metric, out_folder, in_folder, pth=0.05, verbose=False):\n",
    "    \n",
    "    root_path = root_folder + task_folder[numbers[0]] + 'Classification/Results/'\n",
    "    \n",
    "    acc = np.zeros((out_folder*in_folder,len(pipelines)))\n",
    "    for j in range(len(pipelines)):\n",
    "        string_f1 = task_names[numbers[0]] + '_'\n",
    "        string_f2 = '_' + str(srate) +'_' + models[numbers[1]]\n",
    "\n",
    "        lr = GetLearningRateString(models[numbers[1]], task_names[numbers[0]])\n",
    "        \n",
    "        path = root_path + string_f1 + pipelines[j] + string_f2 + '_' + '*' + lr + '*_061_*.pickle'\n",
    "        f = glob.glob(path)\n",
    "        f = sorted(\n",
    "            f,\n",
    "            key = lambda x: (int(x.split(os.sep)[-1].split('_')[4]),\n",
    "                             int(x.split(os.sep)[-1].split('_')[5])\n",
    "                            )\n",
    "        )\n",
    "        if verbose:\n",
    "            print(path)\n",
    "            print(len(f))\n",
    "\n",
    "        if len(f)==0:\n",
    "            if verbose:\n",
    "                print('Empty files @:' + path)\n",
    "        else:\n",
    "            for k in range(len(f)):\n",
    "                with open(f[k], 'rb') as file:\n",
    "                    a = pickle.load(file)\n",
    "                    \n",
    "                acc[k,j] = a[metric]\n",
    "    \n",
    "    if verbose:\n",
    "        print('----Saphiro-Wilk tests for model: ' + models[numbers[1]] + ', task: ' + task_names[numbers[0]] + ' -----')\n",
    "        for j in range(len(pipelines)):\n",
    "            if scipy.stats.shapiro(acc[:,j])[1]<pth:\n",
    "                print('For pipeline: ' + pipelines[j] + ' | the distribution of ' + metric + ' is normal')\n",
    "        print('-------------------------------------------')\n",
    "        print('-------------------------------------------')\n",
    "    med_acc = np.median(acc,0)\n",
    "    \n",
    "    return acc, med_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90485093-84a7-4c6a-828c-20907dd0c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TensorMetrics(root_folder, margin_type, convert_dict, task_folder, task_names, models, pipelines, \n",
    "                      srate, metric, out_folder, in_folder, pth, verbose):\n",
    "                          \n",
    "    margins = ['Task','Model']\n",
    "    \n",
    "    if margin_type == margins[0]:\n",
    "        if verbose:\n",
    "            print('Margin over: Task')\n",
    "        n = len(task_names)\n",
    "        m = len(models)\n",
    "        pairs = []\n",
    "        for j in range(len(models)):\n",
    "            for i in range(len(pipelines)):\n",
    "                for k in range(i+1, len(pipelines)):\n",
    "                    pairs.append([(models[j],convert_dict[pipelines[i]]),(models[j],convert_dict[pipelines[k]])])\n",
    "                        \n",
    "    elif margin_type == margins[1]:\n",
    "        if verbose:\n",
    "            print('Margin over: Model')\n",
    "        n = len(models)\n",
    "        m = len(task_names)\n",
    "        \n",
    "        pairs = []\n",
    "        for j in range(len(task_names)):\n",
    "            for i in range(len(pipelines)):\n",
    "                for k in range(i+1, len(pipelines)):\n",
    "                    pairs.append([(task_names[j],convert_dict[pipelines[i]]),(task_names[j],convert_dict[pipelines[k]])])\n",
    "    \n",
    "    med_acc = np.zeros((n,m,len(pipelines)))\n",
    "    acc_ten = np.zeros((n,m,out_folder*in_folder,len(pipelines)))\n",
    "    if margin_type == margins[0]:\n",
    "        shape_ten = np.shape(acc_ten)\n",
    "        if verbose:\n",
    "            print('Tasks: ', shape_ten[0], '| Models: ',shape_ten[1],'| Folds: ', shape_ten[2], '| Pipelines: ',shape_ten[3])\n",
    "    elif margin_type == margins[1]:\n",
    "        shape_ten = np.shape(acc_ten)\n",
    "        if verbose:\n",
    "            print('Models: ', shape_ten[0], '| Tasks: ',shape_ten[1],'| Folds: ', shape_ten[2], '| Pipelines: ',shape_ten[3])\n",
    "        \n",
    "    for u in range(n):\n",
    "        for i in range(m):\n",
    "            if margin_type == margins[0]:\n",
    "                numbers = [u,i]\n",
    "            elif margin_type == margins[1]:\n",
    "                numbers = [i,u]\n",
    "                \n",
    "            [acc_ten[u,i,:,:] , med_acc[u,i,:]] = median_acc(root_folder, numbers, task_folder, task_names, pipelines, models,\n",
    "                                                             srate, metric, out_folder, in_folder, pth, verbose)\n",
    "    return acc_ten, med_acc, pairs, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1422617-0f53-420b-8d09-548eacf8972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def friedman_test(med_acc, task_or_model_name, pipelines, metric, verbose=False):\n",
    "    \n",
    "    if np.shape(med_acc)[1] == 4:\n",
    "        es = scipy.stats.friedmanchisquare(med_acc[:,0],med_acc[:,1],med_acc[:,2],med_acc[:,3])\n",
    "    elif np.shape(med_acc)[1] == 3:\n",
    "        es = scipy.stats.friedmanchisquare(med_acc[:,0],med_acc[:,1],med_acc[:,2])\n",
    "        \n",
    "    rank_matrix = ((len(pipelines)+1)-scipy.stats.rankdata(med_acc,axis=1)).astype(int)\n",
    "    \n",
    "    N = np.shape(med_acc)[0]\n",
    "    k = len(pipelines)\n",
    "    avg_rank = np.mean(rank_matrix,0)\n",
    "    chi2 = 12*N/(k*(k+1))*np.sum(avg_rank**2) - 3*N*(k+1)\n",
    "    \n",
    "    F = (N-1)*chi2/(N*(k-1)-chi2+np.finfo(np.float32).eps)\n",
    "    \n",
    "    q_05_v = np.array([1.960, 2.343, 2.569, 2.728, 2.850, 2.949, 3.031, 3.102, 3.16])\n",
    "    \n",
    "    if k<=10:\n",
    "        q_05 = q_05_v[k-2]\n",
    "        \n",
    "    CD = q_05*np.sqrt(k*(k+1)/(6*N))\n",
    "\n",
    "    if verbose:\n",
    "        print(es)\n",
    "        print('Rank matrix :')\n",
    "        print(np.round(rank_matrix,3))\n",
    "        print('Avg Rank: ', np.round(avg_rank,3))\n",
    "        print('Chi2: ',np.round(chi2,3))\n",
    "        print('CD: ',np.round(CD,3))\n",
    "        \n",
    "    diff_v = []\n",
    "    for i in range(k):\n",
    "        for j in range(i+1,k):\n",
    "            diff = np.abs(avg_rank[i] - avg_rank[j])\n",
    "            diff_v.append(diff)\n",
    "            if verbose:\n",
    "                if diff >= CD:\n",
    "                    print(pipelines[i] + ' vs ' + pipelines[j],': different, |Ri-Rj|= ',diff)\n",
    "                else:\n",
    "                    print(pipelines[i] + ' vs ' + pipelines[j],': no different, |Ri-Rj|= ',diff)\n",
    "\n",
    "    friedmann_dict = {'name': task_or_model_name,\n",
    "                      'metric': metric,\n",
    "                      'F':F,\n",
    "                      'chi2':chi2,\n",
    "                      'avg_rank':avg_rank,\n",
    "                      'rank_matrix':rank_matrix,\n",
    "                      'CD':CD,\n",
    "                      'diff':diff_v,\n",
    "                      'es':es}\n",
    "\n",
    "    return friedmann_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ec0c4-564a-4b4d-b267-67629afb8c6f",
   "metadata": {},
   "source": [
    "## Statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d61d0-7eea-4851-932d-e2fe3243c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'accuracy_weighted'\n",
    "#metric = 'accuracy_unbalanced'\n",
    "#metric = 'f1score_weighted'\n",
    "#metric = 'precision_weighted'\n",
    "#metric = 'recall_weighted'\n",
    "#metric = 'cohen_kappa'\n",
    "\n",
    "margin_type = 'Model'\n",
    "test_pipes = 'Wilcoxon'\n",
    "corr_method = 'holm'\n",
    "margins = ['Task','Model']\n",
    "\n",
    "# Get Accuracies and Median Accuracies\n",
    "acc_ten, med_acc, pairs, n = get_TensorMetrics(\n",
    "    root_folder, margin_type, convert_dict, task_folder, task_names, models, pipelines,\n",
    "    srate[0], metric, out_folder, in_folder, pth, verbose\n",
    ")\n",
    "\n",
    "# Perform Friedman's Test\n",
    "f = {}\n",
    "for u in range(n):\n",
    "        if margin_type == margins[1]:\n",
    "            f[models[u]] = friedman_test(med_acc[u,:,:], models[u], pipelines, metric)\n",
    "        elif margin_type == margins[0]:\n",
    "            f[task_names[u]] = friedman_test(med_acc[u,:,:], task_names[u], pipelines, metric, verbose)\n",
    "if verbose:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268def5-fbc1-414f-a58d-0c6fb641fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786b31a-b7d9-4ba3-826b-932c9c932227",
   "metadata": {},
   "source": [
    "## Bar-plot Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da5ec1-85cf-4d93-882f-74c2d743c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_modality = 'single_plot' # 'multiple_plots' or 'single_plot'\n",
    "save_img = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193988ad-af34-4df1-a353-48e2f74e94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_or_model(\n",
    "    cdf, feat, ax, u, test_pipes, corr_method, \n",
    "    task_or_model_names, font, save_img,\n",
    "    linew, size, model_or_task_names, metric,\n",
    "    margin_type, letters, convert_dict, verbose\n",
    "):\n",
    "\n",
    "    mdf = pd.melt(cdf, id_vars=[feat], var_name='pipeline')    \n",
    "\n",
    "    sns.boxplot(x=feat, y= 'value', hue='pipeline', data=mdf, showfliers = False, ax=ax,linewidth=linew) \n",
    "    sns.stripplot(x=feat, y=\"value\", data=mdf, legend = False, linewidth=linew,\n",
    "                  hue='pipeline', dodge=True, ax=ax,size=size)\n",
    "    \n",
    "    annotator = Annotator(ax=ax, pairs=pairs, data=mdf, x=feat, hue='pipeline', y='value')\n",
    "    annotator.configure(\n",
    "        test=test_pipes,text_format='star',hide_non_significant=True, verbose=verbose,\n",
    "        loc='inside', comparisons_correction = corr_method,line_width=linew,fontsize=font\n",
    "    )\n",
    "    \n",
    "    annotator.apply_and_annotate()\n",
    "    \n",
    "    ax.set_xticklabels([convert_dict[i] for i in task_or_model_names],fontsize = font-4)\n",
    "    ax.set_title(margin_type + ': ' + convert_dict[model_or_task_names[u]],fontsize = font+3)\n",
    "    ax.set_xlabel(feat,fontsize = font)\n",
    "    ax.set_ylim(15,140)\n",
    "    ax.set_ylabel(convert_dict[metric] + ' %',fontsize = font)\n",
    "    ax.legend(fontsize = font, loc=\"lower left\")\n",
    "    ax.set_yticks(np.arange(15,105,5))\n",
    "    ax.set_yticklabels(np.arange(15,105,5),fontsize = font-4)\n",
    "    ax.text(4.95,135,'('+letters[u]+'-I)',fontsize = font+6)\n",
    "\n",
    "    if len(save_img)!=0:\n",
    "        fig.savefig(save_img + model_or_task_names[u] +'.pdf', transparent=False, bbox_inches='tight')\n",
    "    \n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d4ab8-d788-45f9-85e2-7d7ec3e22db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A','B','C','D','E','F']\n",
    "\n",
    "if plot_modality == 'multiple_plots':\n",
    "    # IF MULTIPLE PLOT\n",
    "    dim_f = 20\n",
    "    fig, ax = plt.subplots(2,2,figsize=(dim_f,dim_f))\n",
    "    font, linew, size = 14, 1, 4\n",
    "    save_img_path = []\n",
    "    \n",
    "elif plot_modality == 'single_plot':\n",
    "    #IF SINGLE PLOT\n",
    "    dim_f = 20\n",
    "    fig, ax = plt.subplots(1,1,figsize=(dim_f,dim_f))\n",
    "    plt.close()\n",
    "    font, linew, size = 34, 3, 10\n",
    "    \n",
    "    if save_img:\n",
    "        save_img_path = images_folder\n",
    "    else:\n",
    "        save_img_path = []\n",
    "\n",
    "try:\n",
    "    ax = ax.flatten()\n",
    "    flag = True\n",
    "except:\n",
    "    ax = ax\n",
    "    flag = False\n",
    "\n",
    "for u in range(n): \n",
    "    if margin_type == margins[1]:\n",
    "        feat = margins[0]\n",
    "        if verbose:\n",
    "            print('------ ',convert_dict[models[u]],' ------')\n",
    "        for i in range(len(task_names)):\n",
    "            df = pd.DataFrame(\n",
    "                acc_ten[u,i,:,:]*100,\n",
    "                columns=[convert_dict[i] for i in pipelines]\n",
    "            ).assign(Task=task_names[i])\n",
    "            if i==0:\n",
    "                cdf = df\n",
    "            else:\n",
    "                cdf = pd.concat([cdf, df])\n",
    "        if flag:\n",
    "            axu = ax[u]\n",
    "        else:\n",
    "            fig, axu = plt.subplots(1,1,figsize=(dim_f,dim_f))\n",
    "            \n",
    "        mdf = plot_task_or_model(\n",
    "            cdf, feat, axu, u, test_pipes,\n",
    "            corr_method, task_names, font, save_img_path,\n",
    "            linew, size, models, metric, margin_type, \n",
    "            letters, convert_dict, verbose\n",
    "        )\n",
    "\n",
    "    elif margin_type == margins[0]:\n",
    "        if verbose:\n",
    "            print('------ ',convert_dict[task_names[u]],' ------')\n",
    "        feat = margins[1]\n",
    "        for i in range(len(models)):\n",
    "            df = pd.DataFrame(\n",
    "                acc_ten[u,i,:,:]*100,\n",
    "                columns=[convert_dict[i] for i in pipelines]\n",
    "            ).assign(Model=models[i])\n",
    "            if i==0:\n",
    "                cdf = df\n",
    "            else:\n",
    "                cdf = pd.concat([cdf, df])         \n",
    "        if flag:\n",
    "            axu = ax[u]\n",
    "        else:\n",
    "            fig, axu = plt.subplots(1,1,figsize=(dim_f,dim_f))\n",
    "            \n",
    "        mdf = plot_task_or_model(\n",
    "            cdf, feat, axu, u, test_pipes,\n",
    "            corr_method, models, font, save_img_path,\n",
    "            linew, size, task_names, metric, margin_type,\n",
    "            letters, convert_dict, verbose\n",
    "        )\n",
    "        \n",
    "    print('--------------------------------------')        \n",
    "\n",
    "if save_img and plot_modality == 'multiple_plots':\n",
    "    fig.savefig(images_folder + margin_type + '.pdf', transparent=False, bbox_inches='tight')\n",
    "    \n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa960cc-55f6-4e27-a03c-736d14b49905",
   "metadata": {},
   "source": [
    "### Boxplot (Task Visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c4b83-9838-4f87-83f8-6de7c592316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'accuracy_weighted'\n",
    "#metric = 'accuracy_unbalanced'\n",
    "#metric = 'f1score_weighted'\n",
    "#metric = 'precision_weighted'\n",
    "#metric = 'recall_weighted'\n",
    "#metric = 'cohen_kappa'\n",
    "\n",
    "margin_type = 'Task'\n",
    "test_pipes = 'Wilcoxon'\n",
    "corr_method = 'holm'\n",
    "margins = ['Task','Model']\n",
    "acc_ten, med_acc, pairs, n = get_TensorMetrics(\n",
    "    root_folder,margin_type, convert_dict,\n",
    "    task_folder, task_names, models, pipelines, \n",
    "    srate[0], metric, out_folder, in_folder, pth, verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee804a85-ebc9-4a50-bf67-705e3d0e8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_modality = 'single_plot' # 'multiple_plots' or 'single_plot\n",
    "save_img = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66546e1e-4e73-4f92-8ec9-64210d367b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A','B','C','D','E','F']\n",
    "\n",
    "if plot_modality == 'multiple_plots':\n",
    "    # IF MULTIPLE PLOT\n",
    "    dim_f = 20\n",
    "    fig, ax = plt.subplots(3,2,figsize=(int(dim_f-5),dim_f))\n",
    "    font, linew, size = 14, 1, 4\n",
    "    save_img_path = []\n",
    "    \n",
    "elif plot_modality == 'single_plot':\n",
    "    #IF SINGLE PLOT\n",
    "    dim_f = 20\n",
    "    fig, ax = plt.subplots(1,1,figsize=(dim_f,dim_f))\n",
    "    plt.close()\n",
    "    font, linew, size = 34, 3, 10\n",
    "    if save_img:\n",
    "        save_img_path = images_folder\n",
    "    else:\n",
    "        save_img_path = []\n",
    "    \n",
    "try:\n",
    "    ax = ax.flatten()\n",
    "    flag = True\n",
    "except:\n",
    "    ax = ax\n",
    "    flag = False\n",
    "\n",
    "pairs_v1 = []\n",
    "for j in range(len(pipelines)):\n",
    "    for i in range(len(models)):\n",
    "        for k in range(i+1, len(models)):\n",
    "            pairs_v1.append([(convert_dict[pipelines[j]],convert_dict[models[i]]),\n",
    "                             (convert_dict[pipelines[j]],convert_dict[models[k]])])\n",
    "\n",
    "for u in range(n):\n",
    "    \n",
    "    if margin_type == margins[0]:\n",
    "        if verbose:\n",
    "            print('------ ',convert_dict[task_names[u]],' ------')        \n",
    "        for i in range(len(pipelines)):\n",
    "            df = pd.DataFrame(\n",
    "                (acc_ten[u,:,:,i].T)*100,\n",
    "                columns=[convert_dict[j] for j in models]\n",
    "            ).assign(Pipelines=convert_dict[pipelines[i]])\n",
    "            if i==0:\n",
    "                cdf = df\n",
    "            else:\n",
    "                cdf = pd.concat([cdf, df])   \n",
    "\n",
    "        mdf = pd.melt(cdf, id_vars=['Pipelines'], var_name='Models')  \n",
    "        \n",
    "        if flag:\n",
    "            axu = ax[u]\n",
    "        else:\n",
    "            fig, axu = plt.subplots(1,1,figsize=(dim_f,int(dim_f-5)))\n",
    "            \n",
    "        sns.boxplot(x='Pipelines', y= 'value', hue='Models', data=mdf, showfliers = False, ax=axu,linewidth=linew) \n",
    "        sns.stripplot(\n",
    "            x='Pipelines', y=\"value\", data=mdf,\n",
    "            legend = False, linewidth=linew,\n",
    "            hue='Models', dodge=True, ax=axu, size=size\n",
    "        )\n",
    "        \n",
    "        annotator = Annotator(\n",
    "            ax=axu, pairs=pairs_v1, data=mdf,\n",
    "            x='Pipelines', hue='Models', y='value'\n",
    "        )\n",
    "        annotator.configure(\n",
    "            test=test_pipes,text_format='star',hide_non_significant=True,\n",
    "            verbose=verbose, loc='inside', comparisons_correction=corr_method,\n",
    "            line_width=linew,fontsize=font\n",
    "        )\n",
    "        annotator.apply_and_annotate()\n",
    "\n",
    "        axu.set_xticklabels([convert_dict[j] for j in pipelines],fontsize = font-4)\n",
    "        axu.set_title(margin_type + ': ' + convert_dict[task_names[u]],fontsize = font+3)\n",
    "        axu.set_xlabel('Pipelines',fontsize = font)\n",
    "        axu.set_ylim(15,140)\n",
    "        \n",
    "    print('--------------------------------------')        \n",
    "    axu.set_ylabel(convert_dict[metric] + ' %',fontsize = font)\n",
    "    if u==0 or u==1:\n",
    "        axu.legend(fontsize = font, loc=\"lower left\")\n",
    "    else:\n",
    "        axu.legend(fontsize = font, loc=\"upper left\")\n",
    "    axu.set_yticks(np.arange(15,105,5))\n",
    "    axu.set_yticklabels(np.arange(15,105,5),fontsize = font-4)\n",
    "    axu.text(3.13,135,'('+letters[u]+')',fontsize = font+6)\n",
    "    \n",
    "    if len(save_img_path)!=0:\n",
    "        fig.savefig(save_img_path + task_names[u] +'.pdf', transparent=False, bbox_inches='tight')\n",
    "    \n",
    "if save_img and plot_modality == 'multiple_plots':\n",
    "    fig.savefig(images_folder + margin_type + '.pdf', transparent=False, bbox_inches='tight')\n",
    "    \n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4d4ee-c141-4122-a683-98f3f8e35443",
   "metadata": {},
   "source": [
    "## CD Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9820929-e2c8-4e32-8a21-ca413b029a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://gist.github.com/janezd and modified\n",
    "def compute_CD(avranks, n, alpha=0.05, test=\"nemenyi\"):\n",
    "    \"\"\"\n",
    "    Returns critical difference for Nemenyi or Bonferroni-Dunn test\n",
    "    according to given alpha (either alpha=\"0.05\" or alpha=\"0.1\") for average\n",
    "    ranks and number of tested datasets N. Test can be either \"nemenyi\" for\n",
    "    for Nemenyi two tailed test or \"bonferroni-dunn\" for Bonferroni-Dunn test.\n",
    "    Args:\n",
    "\tavranks (int or list): list of average ranks or the number of methods\n",
    "\tn (int): number of data sets\n",
    "\talpha (float): alpha level; must be either 0.05 or 0.1\n",
    "\ttest (str): \"nemenyi\" or \"bonferroni-dunn\"\n",
    "    \"\"\"\n",
    "    k = avranks if isinstance(avranks, int) else len(avranks)\n",
    "    d = {(\"nemenyi\", 0.05): [0, 0, 1.959964, 2.343701, 2.569032, 2.727774,\n",
    "                               2.849705, 2.94832, 3.030879, 3.101730, 3.163684,\n",
    "                               3.218654, 3.268004, 3.312739, 3.353618, 3.39123,\n",
    "                               3.426041, 3.458425, 3.488685, 3.517073,\n",
    "                               3.543799],\n",
    "         (\"nemenyi\", 0.1): [0, 0, 1.644854, 2.052293, 2.291341, 2.459516,\n",
    "                              2.588521, 2.692732, 2.779884, 2.854606, 2.919889,\n",
    "                              2.977768, 3.029694, 3.076733, 3.119693, 3.159199,\n",
    "                              3.195743, 3.229723, 3.261461, 3.291224, 3.319233],\n",
    "         (\"bonferroni-dunn\", 0.05): [0, 0, 1.960, 2.241, 2.394, 2.498, 2.576,\n",
    "                                       2.638, 2.690, 2.724, 2.773],\n",
    "         (\"bonferroni-dunn\", 0.1): [0, 0, 1.645, 1.960, 2.128, 2.241, 2.326,\n",
    "                                      2.394, 2.450, 2.498, 2.539]}\n",
    "    q = d[(test, alpha)]\n",
    "    cd = q[k] * (k * (k + 1) / (6.0 * n)) ** 0.5\n",
    "    return cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c6401-f907-4c8f-9ba3-1ff9f72bfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://gist.github.com/janezd and modified\n",
    "def graph_ranks(avranks, names, cd=None, custom_string=None, cdmethod=None, letter=None, lowv=None, highv=None,\n",
    "                width=6, textspace=1, reverse=False, filename=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Draws a CD graph, which is used to display  the differences in methods'\n",
    "    performance. See Janez Demsar, Statistical Comparisons of Classifiers over\n",
    "    Multiple Data Sets, 7(Jan):1--30, 2006.\n",
    "    Needs matplotlib to work.\n",
    "    The image is ploted on `plt` imported using `import matplotlib.pyplot as plt`.\n",
    "    Args:\n",
    "        avranks (list of float): average ranks of methods.\n",
    "        names (list of str): names of methods.\n",
    "        cd (float): Critical difference used for statistically significance of\n",
    "            difference between methods.\n",
    "        cdmethod (int, optional): the method that is compared with other methods\n",
    "            If omitted, show pairwise comparison of methods\n",
    "        lowv (int, optional): the lowest shown rank\n",
    "        highv (int, optional): the highest shown rank\n",
    "        width (int, optional): default width in inches (default: 6)\n",
    "        textspace (int, optional): space on figure sides (in inches) for the\n",
    "            method names (default: 1)\n",
    "        reverse (bool, optional):  if set to `True`, the lowest rank is on the\n",
    "            right (default: `False`)\n",
    "        filename (str, optional): output file name (with extension). If not\n",
    "            given, the function does not write a file.\n",
    "    \"\"\"\n",
    "    width = float(width)\n",
    "    textspace = float(textspace)\n",
    "\n",
    "    def nth(l, n):\n",
    "        n = lloc(l, n)\n",
    "        return [a[n] for a in l]\n",
    "\n",
    "    def lloc(l, n):\n",
    "        if n < 0:\n",
    "            return len(l[0]) + n\n",
    "        else:\n",
    "            return n\n",
    "\n",
    "    def mxrange(lr):\n",
    "        if not len(lr):\n",
    "            yield ()\n",
    "        else:\n",
    "            # it can work with single numbers\n",
    "            index = lr[0]\n",
    "            if isinstance(index, int):\n",
    "                index = [index]\n",
    "            for a in range(*index):\n",
    "                for b in mxrange(lr[1:]):\n",
    "                    yield tuple([a] + list(b))\n",
    "\n",
    "    def print_figure(fig, *args, **kwargs):\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        canvas.print_figure(*args, **kwargs)\n",
    "\n",
    "    sums = avranks\n",
    "\n",
    "    tempsort = sorted([(a, i) for i, a in enumerate(sums)], reverse=reverse)\n",
    "    ssums = nth(tempsort, 0)\n",
    "    sortidx = nth(tempsort, 1)\n",
    "    nnames = [names[x] for x in sortidx]\n",
    "\n",
    "    if lowv is None:\n",
    "        lowv = min(1, int(math.floor(min(ssums))))\n",
    "    if highv is None:\n",
    "        highv = max(len(avranks), int(math.ceil(max(ssums))))\n",
    "\n",
    "    cline = 0.4\n",
    "    k = len(sums)\n",
    "    lines = None\n",
    "    linesblank = 0\n",
    "    scalewidth = width - 2 * textspace\n",
    "\n",
    "    def rankpos(rank):\n",
    "        if not reverse:\n",
    "            a = rank - lowv\n",
    "        else:\n",
    "            a = highv - rank\n",
    "        return textspace + scalewidth / (highv - lowv) * a\n",
    "\n",
    "    distanceh = 0.25\n",
    "\n",
    "    if cd and cdmethod is None:\n",
    "        # get pairs of non significant methods\n",
    "        def get_lines(sums, hsd):\n",
    "            # get all pairs\n",
    "            lsums = len(sums)\n",
    "            allpairs = [(i, j) for i, j in mxrange([[lsums], [lsums]]) if j > i]\n",
    "            # remove not significant\n",
    "            notSig = [(i, j) for i, j in allpairs\n",
    "                      if abs(sums[i] - sums[j]) <= hsd]\n",
    "            # keep only longest\n",
    "\n",
    "            def no_longer(ij_tuple, notSig):\n",
    "                i, j = ij_tuple\n",
    "                for i1, j1 in notSig:\n",
    "                    if (i1 <= i and j1 > j) or (i1 < i and j1 >= j):\n",
    "                        return False\n",
    "                return True\n",
    "\n",
    "            longest = [(i, j) for i, j in notSig if no_longer((i, j), notSig)]\n",
    "\n",
    "            return longest\n",
    "\n",
    "        lines = get_lines(ssums, cd)\n",
    "        linesblank = 0.2 + 0.2 + (len(lines) - 1) * 0.1\n",
    "        # add scale\n",
    "        distanceh = 0.25\n",
    "        cline += distanceh\n",
    "\n",
    "    # calculate height needed height of an image\n",
    "    minnotsignificant = max(2 * 0.2, linesblank)\n",
    "    height = cline + ((k + 1) / 2) * 0.2 + minnotsignificant\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    fig.set_facecolor('white')\n",
    "    ax = fig.add_axes([0, 0, 1, 1])  # reverse y axis\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    hf = 1. / height  # height factor\n",
    "    wf = 1. / width\n",
    "\n",
    "    def hfl(l):\n",
    "        return [a * hf for a in l]\n",
    "    def wfl(l):\n",
    "        return [a * wf for a in l]\n",
    "        \n",
    "    # Upper left corner is (0,0).\n",
    "    ax.plot([0, 1], [0, 1], c=\"w\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)\n",
    "\n",
    "    def line(l, color='k', **kwargs):\n",
    "        \"\"\"\n",
    "        Input is a list of pairs of points.\n",
    "        \"\"\"\n",
    "        ax.plot(wfl(nth(l, 0)), hfl(nth(l, 1)), color=color, **kwargs)\n",
    "\n",
    "    def text(x, y, s, *args, **kwargs):\n",
    "        ax.text(wf * x, hf * y, s, *args, **kwargs)\n",
    "\n",
    "    line([(textspace, cline), (width - textspace, cline)], linewidth=0.7)\n",
    "\n",
    "    bigtick = 0.1\n",
    "    smalltick = 0.05\n",
    "\n",
    "    tick = None\n",
    "    for a in list(np.arange(lowv, highv, 0.25)) + [highv]:\n",
    "        tick = smalltick\n",
    "        if a == int(a):\n",
    "            tick = bigtick\n",
    "        line([(rankpos(a), cline - tick / 2),\n",
    "              (rankpos(a), cline)],\n",
    "             linewidth=0.7)\n",
    "\n",
    "    for a in range(lowv, highv + 1):\n",
    "        text(rankpos(a), cline - tick / 2 - 0.05, str(a),\n",
    "             ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    k = len(ssums)\n",
    "\n",
    "    for i in range(math.ceil(k / 2)):\n",
    "        chei = cline + minnotsignificant + i * 0.2\n",
    "        line([(rankpos(ssums[i]), cline),\n",
    "              (rankpos(ssums[i]), chei),\n",
    "              (textspace - 0.1, chei)],\n",
    "             linewidth=0.7)\n",
    "        text(textspace - 0.2, chei, nnames[i], ha=\"right\", va=\"center\")\n",
    "\n",
    "    for i in range(math.ceil(k / 2), k):\n",
    "        chei = cline + minnotsignificant + (k - i - 1) * 0.2\n",
    "        line([(rankpos(ssums[i]), cline),\n",
    "              (rankpos(ssums[i]), chei),\n",
    "              (textspace + scalewidth + 0.1, chei)],\n",
    "             linewidth=0.7)\n",
    "        text(textspace + scalewidth + 0.2, chei, nnames[i],\n",
    "             ha=\"left\", va=\"center\")\n",
    "\n",
    "    if cd and cdmethod is None:\n",
    "        # upper scale\n",
    "        if not reverse:\n",
    "            begin, end = rankpos(lowv), rankpos(lowv + cd)\n",
    "        else:\n",
    "            begin, end = rankpos(highv), rankpos(highv - cd)\n",
    "\n",
    "        line([(begin, distanceh), (end, distanceh)], linewidth=0.7)\n",
    "        line([(begin, distanceh + bigtick / 2),\n",
    "              (begin, distanceh - bigtick / 2)],\n",
    "             linewidth=0.7)\n",
    "        line([(end, distanceh + bigtick / 2),\n",
    "              (end, distanceh - bigtick / 2)],\n",
    "             linewidth=0.7)\n",
    "        text((begin + end) / 2, distanceh - 0.05, \"CD\" + \" | \" + custom_string,\n",
    "             ha=\"center\", va=\"bottom\")\n",
    "        text(end+1.5,distanceh - 0.05,'('+letter+'-II)')\n",
    "\n",
    "        # no-significance lines\n",
    "        def draw_lines(lines, side=0.05, height=0.1):\n",
    "            start = cline + 0.2\n",
    "            for l, r in lines:\n",
    "                line([(rankpos(ssums[l]) - side, start),\n",
    "                      (rankpos(ssums[r]) + side, start)],\n",
    "                     linewidth=2.5)\n",
    "                start += height\n",
    "\n",
    "        draw_lines(lines)\n",
    "\n",
    "    elif cd:\n",
    "        begin = rankpos(avranks[cdmethod] - cd)\n",
    "        end = rankpos(avranks[cdmethod] + cd)\n",
    "        line([(begin, cline), (end, cline)],\n",
    "             linewidth=2.5)\n",
    "        line([(begin, cline + bigtick / 2),\n",
    "              (begin, cline - bigtick / 2)],\n",
    "             linewidth=2.5)\n",
    "        line([(end, cline + bigtick / 2),\n",
    "              (end, cline - bigtick / 2)],\n",
    "             linewidth=2.5)\n",
    "\n",
    "    if filename:\n",
    "        #print_figure(fig, filename, **kwargs)\n",
    "        fig.tight_layout()\n",
    "        plt.margins(0,0)\n",
    "        fig.savefig(filename +'_CD.pdf', transparent=False, bbox_inches='tight',pad_inches=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc592f6d-50f1-4b43-8c7e-6eb9aca46d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img = True\n",
    "\n",
    "#metric = 'accuracy_unbalanced'\n",
    "metric = 'accuracy_weighted'\n",
    "#metric = 'f1score_weighted'\n",
    "#metric = 'precision_weighted'\n",
    "#metric = 'recall_weighted'\n",
    "#metric = 'cohen_kappa'\n",
    "\n",
    "posthoc_test = \"nemenyi\"\n",
    "margin_type = 'Model'\n",
    "margins = ['Task','Model']\n",
    "acc_ten, med_acc, pairs, n = get_TensorMetrics(\n",
    "    root_folder, margin_type, convert_dict,\n",
    "    task_folder, task_names, models, pipelines, \n",
    "    srate[0], metric, out_folder, in_folder, pth, verbose\n",
    ")\n",
    "\n",
    "f = {}\n",
    "for u in range(n):\n",
    "        if margin_type == margins[1]:\n",
    "            f[models[u]] = friedman_test(med_acc[u,:,:], models[u], pipelines, metric)\n",
    "        elif margin_type == margins[0]:\n",
    "            f[task_names[u]] = friedman_test(med_acc[u,:,:], task_names[u], pipelines, metric)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488c603-22ea-4ad8-8a42-a52bad86cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if margin_type == margins[1]:\n",
    "    for u in range(n):\n",
    "        cd = compute_CD(\n",
    "            f[models[u]]['avg_rank'], len(task_names), alpha=pth, test=posthoc_test\n",
    "        )\n",
    "        s = margin_type + ': ' + convert_dict[models[u]]\n",
    "        if save_img:\n",
    "            filename = images_folder + models[u]\n",
    "        else:\n",
    "            filename = None\n",
    "        graph_ranks(\n",
    "            f[models[u]]['avg_rank'],\n",
    "            [convert_dict[i] for i in pipelines],\n",
    "            cd, s, filename = filename, letter=letters[u]\n",
    "        )\n",
    "        \n",
    "elif margin_type == margins[0]:\n",
    "    for u in range(n):\n",
    "        cd = compute_CD(f[task_names[u]]['avg_rank'], len(models), alpha=pth, test=posthoc_test)\n",
    "        s = margin_type + ': ' + task_names[u]\n",
    "        \n",
    "        if save_img:\n",
    "            filename = images_folder + task_names[u]\n",
    "        else:\n",
    "            filename = None\n",
    "        graph_ranks(\n",
    "            f[task_names[u]]['avg_rank'],\n",
    "            [convert_dict[i] for i in pipelines],\n",
    "            cd, s, filename = filename,letter=letters[u]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613ef9c-b49e-4f81-b451-556ff4e2e1d4",
   "metadata": {},
   "source": [
    "## Pie-Charts Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd85775-21e9-440e-ba03-b7e327d4d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "save_img = True\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(10,10))\n",
    "fig.tight_layout()\n",
    "axs = ax.flatten()\n",
    "font=17\n",
    "fig.suptitle('Best Performing Pipeline',fontsize=font+6,y=1.05)\n",
    "\n",
    "Piecounts = np.zeros((len(models),len(pipelines)))\n",
    "for u in range(len(models)):\n",
    "    for i in range(len(task_names)):\n",
    "        for z in range(in_folder*out_folder):\n",
    "            acc_ten_pipes = acc_ten[u,i,z,:]\n",
    "            Piecounts[u, int(np.argmax(acc_ten_pipes))] += 1\n",
    "    ax = axs[u]\n",
    "    patches, texts, autotexts = ax.pie(\n",
    "        Piecounts[u,:], autopct='%1.1f%%',\n",
    "        textprops={'size': font+2}, labeldistance=1.08\n",
    "    )\n",
    "    if u>=2:\n",
    "        ax.set_title('Model: ' + convert_dict[models[u]], fontsize=font+4, y=-0.03)\n",
    "    else:\n",
    "        ax.set_title('Model: ' + convert_dict[models[u]], fontsize=font+4, y=0.97)\n",
    "        \n",
    "    if u==0:\n",
    "        ax.legend([convert_dict[j] for j in pipelines],fontsize=font,bbox_to_anchor=(0.81, 0.15))\n",
    "    ax.axis('equal')\n",
    "    \n",
    "if save_img:\n",
    "    fig.savefig(images_folder + 'Model_PIE.pdf', transparent=False, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
